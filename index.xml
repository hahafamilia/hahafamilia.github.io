<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Developement and Life Blog on haha family&#39;s happy blog</title>
    <link>https://hahafamilia.github.io/</link>
    <description>Recent content in Developement and Life Blog on haha family&#39;s happy blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko-kr</language>
    <lastBuildDate>Mon, 16 Sep 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://hahafamilia.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Cloudera CDH 6.1.1 설치하기</title>
      <link>https://hahafamilia.github.io/bigdata/cloudera-cdh-6_1-intall/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hahafamilia.github.io/bigdata/cloudera-cdh-6_1-intall/</guid>
      <description>ASIS 플랫폼의 고도화 일환으로 빅데이터 플랫폼을 신규로 구축하게 되었어요. 구축과정의 일환인 Cloudera CDH 6.1.1 설치과정을 뒤늦게사나마 정리해볼게요. 설치를 진행하기 전에 플랫폼 아키텍처의 설계와 물리서버 사양의 선택, 랙 배치 등이 우선되었겠죠? 이것에 대해서는 또 정리하도록 할게요.
설치 과정은 크게 3단계로 진행되요.
 설치하기 전에 Cloudera Manager 설치 CDH 구성요소 설치  Cloudera CDH 6.1 버전의 공식 문서를 참고해서 진행했어요.
Cloudera Enterprise 6.1 Document
Cloudera Installation Guide
버전  CentOS 7.6.1810 Java 1.</description>
    </item>
    
    <item>
      <title>Kafka Manager 설치</title>
      <link>https://hahafamilia.github.io/bigdata/kafka-manager-installation/</link>
      <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hahafamilia.github.io/bigdata/kafka-manager-installation/</guid>
      <description>KafkaManager 설치 Kafka Manager 를 설치해 볼게요. Kafka Manager 는 Yahoo 의 오픈소스 인데, Kafka 서비스의 상태를 확인하거나, Skew 등이 발생했을때 Reassign Partitoin 등을 할 수 있는 기능을 제공해줘요.
https://hahafamilia.github.io/bigdata/kafka-broker-reinstall/ Github 에서 다운로드 한 후, Scala 빌드 빌드툴 sbt 로 빌드 해줘요.
tar -xvzf kafka-manager-1.3.3.22.tar.gz cd kafka-manager-1.3.3.22 PATH=/usr/java/jdk1.8.0_141-cloudera/bin:$PATH JAVA_HOME=/usr/java/jdk1.8.0_141-cloudera \ ./sbt -java-home /usr/java/jdk1.8.0_141-cloudera clean dist cp target/universal/kafka-manager-1.3.3.22.zip /usr/local cd /usr/local unzip kafka-manager-1.3.3.22.zip  conf/application.conf 설정파일에 Zookeeper 호스트 주소를 설정해 줘요.</description>
    </item>
    
    <item>
      <title>필리핀 세부 마리바고 블루워터 여름휴가</title>
      <link>https://hahafamilia.github.io/life/travel-cebu-2019/</link>
      <pubDate>Fri, 27 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hahafamilia.github.io/life/travel-cebu-2019/</guid>
      <description>이번 여름 휴가는 필리핀 세부(Cebu) 막탄으로 다녀오게되었어요. 향공권, 마리바고 블루워터 리조트, 세부날씨, 그랩, 유심칩, 모닝글로리, 탑스그릴, 김떡순, 예스마트, 더테라스, 오스파, 플라워트리, 그랜드네일, 보노보노 구매대행, 그리고 브라질리언 왁싱 등의 여행을 준비하면서 다녀오고 나서의 후기를 간략히 남겨보려합니다.
2019년 09월, 필리핀 세부, 블루워터 마리바고 여름휴가 어른 2명, 5살 아이 1명, 돌된 아이 1명 이렇게 4인 가족의 9월 21 ~ 9월 26 일까지의 일정 이었어요. 가기전에 날씨 검색해보니 이틀 정도 소나기 예상이였는데, 여행 기간 중에 비가 오진 않았어요.</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>https://hahafamilia.github.io/about/</link>
      <pubDate>Mon, 16 Sep 2019 15:26:00 +0900</pubDate>
      
      <guid>https://hahafamilia.github.io/about/</guid>
      <description>Juil Cho About Me 

Email octchristmas@naver.com
GitHub https://github.com/hahafamilia
이전 블로그 https://tjstory.tistory.com/</description>
    </item>
    
    <item>
      <title>휴고 설치 및 설정, Learn 테마, Hugo Website</title>
      <link>https://hahafamilia.github.io/howto/hugo-staticgen/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hahafamilia.github.io/howto/hugo-staticgen/</guid>
      <description>Jekyll 을 사용하다 이번에 Hugo 로 블로그를 이전하게 되었어요. Jekyll 과 Hugo 를 비교하는 글들이 많은데, 제 이유는 Category 관리가 Hugo 가 좀 더 직관적이여서 예요.
Hugo 를 설치하고 Learn Theme 를 적용하는 방법을 살펴볼게요. 또 Disqus 댓글, 검색어 노출을 위해 Google Analystics, Google Search, 네이버 웹마스터와 연동하는 방법, 초안(Draft) 작성을 위한 설정 등을 살펴 볼게요.
Environment &amp;amp; Requirement  Hugo Static Site Generator v0.57.2 Mac Git  Quick Start Install Mac 에서 Hugo Install Doc 따라서 설치는 쉽게 진행 가능해요.</description>
    </item>
    
    <item>
      <title>Apache Zeppelin Interpreter, Hive, Impala</title>
      <link>https://hahafamilia.github.io/bigdata/zeppelin-interpreter/</link>
      <pubDate>Thu, 29 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hahafamilia.github.io/bigdata/zeppelin-interpreter/</guid>
      <description>Apache Zeppelin 과 Cloudera CDH 의 Hive, Impala 를 연동하는 방법을 알아볼게요.
Zeppelin Posts  Zeppelin Install &amp;amp; QuickStart Zeppelin Usage Zeppelin interpreter  Environment  Oracle JDK 1.8 CentOS 7 Zepplin 0.8.1 Cloudera CDH 6.1.1 Hive 2.1.1 Impala 3.1.0 Impala JDBC Driver 2.6.12  Hive Zeppelin Hive Interpreter Document 문서를 보면 Jdbc Interpreter 를 사용하라고 되어있네요. 기본으로 PostgreSQL Connector 지원하고 그외는 Connector 는 추가를 해줘야 해요.
Maven Repository Zeppelin 콘솔에서 우측 상단의 메뉴에서 interpreter 메뉴를 클릭하여 Interpreter 설정 화면으로 이동해요.</description>
    </item>
    
    <item>
      <title>Apache Zeppelin Usage</title>
      <link>https://hahafamilia.github.io/bigdata/zeppelin-usage/</link>
      <pubDate>Wed, 28 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hahafamilia.github.io/bigdata/zeppelin-usage/</guid>
      <description>Apache Zeppelin 은 노트북 방식의 시각화 툴이예요.
Zeppelin Posts  Zeppelin Install &amp;amp; QuickStart Zeppelin Usage Zeppelin interpreter  Environment  Oracle JDK 1.8 CentOS 7 Zepplin 0.8.1  Dynamic Form Zeppelin 에서는 Dynamic Form 을 제공하고 있어서 Form 을 통해 입력받은 값으로 조건을 주는 형태로 사용 가능해요. Dynamic Form 은 Paragraph scope 와 Note scope 에서 사용 문법의 차이가 있고, 또한 Programmatically 하게 추가하실 수 있어요. 예제에서는 text, select, checkbox 를 소개하고 있네요.</description>
    </item>
    
    <item>
      <title>Apache Zeppelin 설치, QuickStart</title>
      <link>https://hahafamilia.github.io/bigdata/zeppelin-quickstart/</link>
      <pubDate>Wed, 21 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hahafamilia.github.io/bigdata/zeppelin-quickstart/</guid>
      <description>Apache Zeppelin 은 노트북 방식의 시각화 툴이예요. 다양한 시각화 툴이 존재하지만 &amp;lsquo;가장 좋은 것&amp;rsquo;이 아니라 &amp;lsquo;나에게 맞는 것&amp;rsquo; 을 선택했어요. 제가 Zeppelin을 선택한 이유는 아래와 같아요.
 설치와 사용법이 쉬워야 한다. 요구사항을 유연하게 처리 할 수 있어야 한다. BI 웹 어드민을 개발 하지 않아도 되도록 정적 HTML 을 제공해야 한다.  Zeppelin Posts  Zeppelin Install &amp;amp; QuickStart Zeppelin Usage Zeppelin interpreter  Environment  Oracle JDK 1.8 CentOS 7 Zepplin 0.</description>
    </item>
    
    <item>
      <title>Oozie Workflow Email 알림 설정</title>
      <link>https://hahafamilia.github.io/bigdata/oozie-workflow-email/</link>
      <pubDate>Wed, 14 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hahafamilia.github.io/bigdata/oozie-workflow-email/</guid>
      <description>Oozie 에서 Workflow 의 결과에 따른 Email 알림을 받는 방법에 대해서 알아볼게요.
Workflow 를 1회성으로 실행시키는 경우와 Schedule 로 등록하여 주기적으로 실행하는 경우가 있을 수 있을텐데요. 1회성으로 Workflow 만 실행할때는 성공/실패에 대한 처리 결과를 받도록 하고, Schedule 로 등록하여 주기적으로 실행하는 경우에는 실패에 대한 알림 만을 받도록 합니다.
Version  Cloudera 6.1.1 Oozie 5.0.0 Hue 4.3.0  Oozie SMTP 설정 Cloudera Manager &amp;gt; Oozie &amp;gt; 구성 탭 에서 mail 을 검색해서, oozie.</description>
    </item>
    
    <item>
      <title>AvroFlumeEvent, 이벤트 데이터의 발생 시각에 따른 데이터 수집</title>
      <link>https://hahafamilia.github.io/bigdata/spring-kafka-flume/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hahafamilia.github.io/bigdata/spring-kafka-flume/</guid>
      <description>다음과 같은 데이터 파이프 라인을 가정해 볼게요.
 사용자의 Activity 이벤트가 발생 API 서버를 통해 Kafka 에 Produce Flume 을 통해 Kafka 의 메시지를 HDFS 로 적재 HDFS 에는 일자 별로 생성된 디렉토리에 저장  Flume 을 퉁한 데이터 수집시에 Hdfs Sink 는 useLocalTimestamp 설정은 제공 해요. 하지만 이 설정은 이벤트의 수집 시각을 기준으로 해요.
예제 그림에서 API 에서 시작된 데이터는 파이프라인을 거쳐 HDFS 에 도달하기까지 2초의 시간이 소요된다고 가정하면, 2019-08-01 23:59:59 시각에 발생한 이벤트는 useLocalTimestamp 설정에 의해 2019-08-02 00:00:01 의 시각으로 2019-08-02 디렉토리에 적재됩니다.</description>
    </item>
    
    <item>
      <title>Cloudera Manager 알람 설정, Gmail SMPT 서버 사용</title>
      <link>https://hahafamilia.github.io/bigdata/cloudera-alert-email/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hahafamilia.github.io/bigdata/cloudera-alert-email/</guid>
      <description>Cloudera Manager 알림을 보내는 방법은 SMTP, SNMP, Custom Script 세가지를 제공하고 있어요. 여기서 SNMP, Custom Script 는 Enterprise 버전에서만 지원해요. 별도의 SMTP 서버를 운영하고 있지 않다면 Gmail 을 이용할 수 있어요.
Cloudera Version Cloudera 6.1
Gmail 설정 우선 Gmail 설정에서 IMAP 사용이 허용되어야 해요. 그리고 Google 계정 설정 &amp;gt; 보안 &amp;gt; 보안 수준이 낮은 앱의 액세스 가 허용되어야 합니다.
 이 방법은 보안상 권장하지 않는 방법 이예요. 하지만 제가 근무하는 회사의 G Suite 에서는 액세스키에 의한 접근 기능이 제공되고 있지 안아서 이 방법을 사용하고 있어요.</description>
    </item>
    
    <item>
      <title>Python 개발환경, Pyenv, Anaconda3</title>
      <link>https://hahafamilia.github.io/python/python-pyenv-anaconda-mac/</link>
      <pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hahafamilia.github.io/python/python-pyenv-anaconda-mac/</guid>
      <description>Mac 에서 Homebrew 통해서 Python 가상화 환경을 구성하려고 합니다. 우선 Mac 버전이 모하비라면 모하비 brew error 글을 한번 읽어보세요.
Python 가상환경 구성방법은 여러가지가 있지만, pyenv/virtualenv 를 사용할거예요. 대략적인 과정은 pyenv 설치, anaconda3 설치, 가상환경 구성의 순서예요. pyenv 는 가상화 관리를 위해서 pyenv-virtual 패키지를 사용하는데, anaconda3 정도만 사요할 예정이면 설치 하지 않아요 되요.
pyenv brew help brew update brew install pyenv pyenv -v pyenv 1.2.13 pyenv install -list | grep anaconda .</description>
    </item>
    
    <item>
      <title>Hive Java UDF, 유니코드</title>
      <link>https://hahafamilia.github.io/bigdata/hive-java-udf/</link>
      <pubDate>Fri, 19 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hahafamilia.github.io/bigdata/hive-java-udf/</guid>
      <description>개요 Cloudera Document 6.1 Hive UDF 문서를 참고하여 Cloudera CDH 플랫폼에서 HIVE UDF 를 작성하는 방법을 알아봅니다.
또한 MySQL의 Collate 와 문자열 유니코드에 대해서도 간단히 알아보겠습니다.
저의 이번 경우는 HIVE로 집계된 데이터를 SQOOP으로 export 시에 오류가 발생하였습니다.
기존에 설계된 MySQL 디비의 테이블 칼럼 Collate 속성이 utf8-general-ci 로 설계되어 있어 Key 칼럼에 Accent 문자열을 포함하는 문자열 데이터 자정시에 상황에 따라 키중복이 발생했던 것이었습니다.
칼럼의 Collate 속성이 utf8-general-ci 일경우 아래와 같은 경우 ã 문자열은 저장이 되겠지만 a 문자열 저장시 Key 중복 오류가 발생합니다.</description>
    </item>
    
    <item>
      <title>Kafka Broker 디스크 증설, RAID구성, OS 재설치</title>
      <link>https://hahafamilia.github.io/bigdata/kafka-broker-reinstall/</link>
      <pubDate>Tue, 25 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hahafamilia.github.io/bigdata/kafka-broker-reinstall/</guid>
      <description>Kafka Broker 재설치 Kafka Broker 서버의 디스크 용량을 증설하는 작업을 진행 하게 되었습니다. RAID 10으로 디스크 구성을 변경하다보니 불가피하게 OS를 재설치 합니다. Kafka는 Broker 서버의 장애 상황에서도 서비스를 유지 할 수 있도록 설계되어 있어요. Broker 서버를 1대씩 순차적으로 RAID 구성 및 OS 재설치 진행후에 Partition을 Reassign 할 계획입니다.
Environment  CentOS 7.6 Cloudera CDH 6.1.1 Cloudera Manager Kafka 2.0.0-cdh6.1.1 Kafka Manager Kafka Broker 3대 Replication fector 3  HowTo Cloudera Manager 에서 작업대상 Broker 서버를 서비스에서 제거해요.</description>
    </item>
    
    <item>
      <title>Markdown 작성법</title>
      <link>https://hahafamilia.github.io/howto/markdown/</link>
      <pubDate>Thu, 20 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hahafamilia.github.io/howto/markdown/</guid>
      <description># header ## header ### header #### header ##### header ###### header  header header header header header header *** --- ===  ===
# header ## header ### header #### header ##### header ###### header  header header header header header header 1. List 1. List 1. Sub List 1. List  * List * List * List * List - List + List   List  List  List   List List List</description>
    </item>
    
    <item>
      <title>하이브 핵심 정리, Apache Hive Essentials</title>
      <link>https://hahafamilia.github.io/book/%ED%95%98%EC%9D%B4%EB%B8%8C-%ED%95%B5%EC%8B%AC%EC%A0%95%EB%A6%AC/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hahafamilia.github.io/book/%ED%95%98%EC%9D%B4%EB%B8%8C-%ED%95%B5%EC%8B%AC%EC%A0%95%EB%A6%AC/</guid>
      <description>하둡 기반 대용량 데이터 저장, 관리의 핵심 솔루션 라는 부제를 가지고 있는 이 책에서는 HIVE(1.0.0) 에 대해서 설명하고, HQL, 성능, 보안, 다른 툴과의 연동에 대해서 설명하고 있어요.
교보문고 링크
출판  저자 : 다융 두, 김용환 옮김 출판사 : 에이콘출판 출간일 : 2017.02.28  목차  1장. 빅데이터와 하이브 소개 2장. 하이브 환경 설정 3장. 데이터 정의와 설명 4장. 데이터 선택과 범위 5장. 데이터 조작 6장. 데이터 집계와 샘플링 7장.</description>
    </item>
    
    <item>
      <title>카프카, 데이터 플랫폼의 최강자</title>
      <link>https://hahafamilia.github.io/book/%EC%B9%B4%ED%94%84%EC%B9%B4-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%94%8C%EB%9E%AB%ED%8F%BC%EC%9D%98-%EC%B5%9C%EA%B0%95%EC%9E%90/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hahafamilia.github.io/book/%EC%B9%B4%ED%94%84%EC%B9%B4-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%94%8C%EB%9E%AB%ED%8F%BC%EC%9D%98-%EC%B5%9C%EA%B0%95%EC%9E%90/</guid>
      <description>실시간 비동기 스트리밍 솔루션 Kafka의 기본부터 확장 응용까지 라는 부제를 가지고 있는 이 책에서는 카프카 메시지 큐(버전 1.0.0)의 컨슈머, 프로듀서, 주키퍼, 카프카 모니터링, 카프카 매니저 등의 개념과 운영에 대해서 설명하고, 확장하여 카프카 스트림즈 API, 카프카SQL, 파일비트, 나이파이, 키반, 엘라스틱서치, 구글 펍/섭, 아마존 키네시스, 도커 등의 키워드를 설명하고 있어요.
교보문고 링크
출판  저자 : 고승범, 공용준 출판사 : 책만 출간일 : 2018.04.26  목차  1부 카프카를 시작하며  1장 카프카란 무엇인가 2장 카프카와 주키퍼 설치  2부 기본 개념과 운영 가이드  카프카 디자인 카프카 프로듀서 카프카 컨슈머 카프카 운영 가이드  3부 카프카의 확장과 응용  카프카를 활용한 데이터 파이프라인 구축 카프카 스트림즈 API 카프카 SQL을 이용한 스트리밍 처리 그 밖의 클라우드 기반 메시징 서비스  부록 도커를 이용한 카프카 설치  개인평 빅데이터 플랫폼 고도화 과정의 일환으로 RabbitMQ 3.</description>
    </item>
    
    <item>
      <title>모든 주식을 소유하라</title>
      <link>https://hahafamilia.github.io/book/%EB%AA%A8%EB%93%A0%EC%A3%BC%EC%8B%9D%EC%9D%84%EC%86%8C%EC%9C%A0%ED%95%98%EB%9D%BC/</link>
      <pubDate>Sat, 26 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hahafamilia.github.io/book/%EB%AA%A8%EB%93%A0%EC%A3%BC%EC%8B%9D%EC%9D%84%EC%86%8C%EC%9C%A0%ED%95%98%EB%9D%BC/</guid>
      <description>Description 세계 4대 투자의 거장, 존 보글의 투자 법칙.
2015년 워렌 버핏이 주주들에게 주천한 책!
교보문고
우화, 고트락스 가문 고트락스라는 부자 가문이 여러 대에 걸쳐 번창하여 형제, 자매, 삼촌, 사촌이 수천명이나 되었고, 미국의 모든 주식을 100% 소유하게 되었다. 얼마 후 브로커가 나타나서 다른 친척들보다 돈을 더 많이 벌 수 있는 방법이 있다고 속삭였다. 브로커들은 친척에게 보유 주식을 사고 팔게 했다. 브로커들은 대가로 수수료를 받고, 가문 사람들은 주식을 거래하는데 발생하는 자본이득에 대해서도 세금을 내야 했다.</description>
    </item>
    
    <item>
      <title>Jekyll, Github.io, Minimal mistakes 블로그 만들기, 목차 한글링크 버그</title>
      <link>https://hahafamilia.github.io/howto/jekyll-github-mistakes-blog/</link>
      <pubDate>Fri, 25 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://hahafamilia.github.io/howto/jekyll-github-mistakes-blog/</guid>
      <description>개발자 초창기에는 블로그(이전의 블로그) 활동도 열심히 했었는데&amp;hellip; 너무 잊고 살았었네요. 새롭게 블로그를 시작 할 생각에 Jekyll, Github.io 로 블로그를 구성하게 되었어요.
간단히 Jekyll 은 정적파일 생성기, Github.io 는 호스팅, Minimal mistakes 는 수많은 Jekyll 테마중의 하나예요.
선택의 기준은 markdown 에디터 때문이에요. 개발자는 에디터 툴을 가장 많이 다루는데요. 아무래도 markdown 으로 글을 작성하고 Git 으로 Push 하여 포스팅을 하는 구조라면 블로그 활동량이 많아지지 않을까 합니다.
그럼 Github.io, Jekyll, Minimal mistakes theme 를 이용해 블로그를 만들어 봐요.</description>
    </item>
    
    <item>
      <title>하둡 애플리케이션 아키텍처</title>
      <link>https://hahafamilia.github.io/book/%ED%95%98%EB%91%A1-%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://hahafamilia.github.io/book/%ED%95%98%EB%91%A1-%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/</guid>
      <description>하둡 에코시스템을 활용한 빅데이터 처리 라는 부제을 가지고 있는 이 책에서는 HDFS/HBASE 스키마 디자인, 데이터 파이프라인, 맵리듀스, 스파크, 피크, 크런치, 하이브, ETL, 스트리밍, 근접 실시간, 데이터 웨어하우스, 사례연구 등 데이터의 수집에서 분석까지의 빅데이터 아키텍처에 대해 설명하고 있어요.
교보문고 링크
출간  저자 : 마크 그로버, 테드 멀래스커, 조나단 사이드먼, 그웬 사피라 옮김 : 정동식, 홍다경, 우지현 출판사 : 비제이퍼블릭 출간일 : 2016.05.30  목차  1부. 하둡 애플리케이션의 아키텍처 고려사항  1장.</description>
    </item>
    
  </channel>
</rss>